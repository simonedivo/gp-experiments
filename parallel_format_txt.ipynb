{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimezzato il numero di seed per ora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"new_random_seeds.txt\", \"r\")\n",
    "random_seeds = file.read()\n",
    "file.close()\n",
    "\n",
    "random_seeds = random_seeds.replace(\"\\n\", \"\").replace(\" \", \"\").split(\",\")\n",
    "#random_seeds = [int(seed) for seed in random_seeds]\n",
    "\n",
    "datasets_folder = './datasets/'\n",
    "datasets = ['1191_BNG_pbc.tsv', '1196_BNG_pharynx.tsv', '1595_poker.tsv']\n",
    "\n",
    "traning_set_dimension_list = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]\n",
    "big_traning_set_dimension_list = [100000, 250000, 500000]\n",
    "popsize_list = [10, 20, 50, 100]\n",
    "big_popsize_list = [200, 500, 1000, 2000, 5000, 10000]\n",
    "generation_list = [10, 20, 50, 100]\n",
    "big_generation_list = [200, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "input_args = \"\"\n",
    "\n",
    "#T-P-G fino a 50000,100,100, fino a 8640 -> FATTO \n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{popsize},{generation}\\n\"\n",
    "\n",
    "#BT-P-G fino a 500000,100,100, fino a 10800 -> FATTO\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{popsize},{generation}\\n\"\n",
    "\n",
    "#T-BP-G fino a 50000,10000,100, fino a 23760 -> FATTO\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{big_popsize},{generation}\\n\"\n",
    "\n",
    "#BT-BP-G fino a 500000,10000,100, fino a 27000 -> IN CORSO SU CM10 E GOOGLE\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{big_popsize},{generation}\\n\"\n",
    "\n",
    "#T-P-BG fino a 50000,100,10000, fino a 39960 -> IN TEORIA FATTO\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{popsize},{big_generation}\\n\"\n",
    "\n",
    "#BT-P-BG fino a 500000,100,10000, fino a 43200 -> DA FARE SU ULYSSES\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{popsize},{big_generation}\\n\"\n",
    "\n",
    "#T-BP-BG fino a 50000,10000,10000, fino a 62640\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{big_popsize},{big_generation}\\n\"\n",
    "\n",
    "#BT-BP-BG fino a 500000,10000,10000, fino a 67500\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{big_popsize},{big_generation}\\n\"\n",
    "\n",
    "#print(input_args)\n",
    "\n",
    "with open(\"all_input_args.txt\", \"w\") as file:\n",
    "    file.write(input_args)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per Ulysses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"new_random_seeds.txt\", \"r\")\n",
    "random_seeds = file.read()\n",
    "file.close()\n",
    "\n",
    "random_seeds = random_seeds.replace(\"\\n\", \"\").replace(\" \", \"\").split(\",\")\n",
    "#random_seeds = [int(seed) for seed in random_seeds]\n",
    "\n",
    "datasets_folder = './datasets/'\n",
    "datasets = ['1191_BNG_pbc.tsv', '1196_BNG_pharynx.tsv', '1595_poker.tsv']\n",
    "\n",
    "traning_set_dimension_list = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]\n",
    "big_traning_set_dimension_list = [100000, 250000, 500000]\n",
    "popsize_list = [10, 20, 50, 100]\n",
    "big_popsize_list = [200, 500, 1000, 2000, 5000, 10000]\n",
    "generation_list = [10, 20, 50, 100]\n",
    "big_generation_list = [200, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "input_args = \"\"\n",
    "\n",
    "#T-P-G fino a 50000,100,100, numero 8640 -> FATTO\n",
    "#for seed in random_seeds:\n",
    "#    for dataset in datasets:\n",
    "#        for training_set_dimension in traning_set_dimension_list:\n",
    "#            for popsize in popsize_list:\n",
    "#                for generation in generation_list:\n",
    "#                    input_args += f\"{seed},{dataset},{training_set_dimension},{popsize},{generation}\\n\"\n",
    "#\n",
    "##BT-P-G fino a 500000,100,100, numero 10800 -> FATTO\n",
    "#for seed in random_seeds:\n",
    "#    for dataset in datasets:\n",
    "#        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "#            for popsize in popsize_list:\n",
    "#                for generation in generation_list:\n",
    "#                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{popsize},{generation}\\n\"\n",
    "#\n",
    "##T-BP-G fino a 50000,10000,100, numero 23760 -> IN CORSO\n",
    "#for seed in random_seeds:\n",
    "#    for dataset in datasets:\n",
    "#        for training_set_dimension in traning_set_dimension_list:\n",
    "#            for big_popsize in big_popsize_list:\n",
    "#                for generation in generation_list:\n",
    "#                    input_args += f\"{seed},{dataset},{training_set_dimension},{big_popsize},{generation}\\n\"\n",
    "\n",
    "##BT-BP-G fino a 500000,10000,100, numero 27000\n",
    "#for seed in random_seeds:\n",
    "#    for dataset in datasets:\n",
    "#        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "#            for big_popsize in big_popsize_list:\n",
    "#                for generation in generation_list:\n",
    "#                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{big_popsize},{generation}\\n\"\n",
    "#\n",
    "#T-P-BG fino a 50000,100,10000, numero 39960\n",
    "#for seed in random_seeds:\n",
    "#    for dataset in datasets:\n",
    "#        for training_set_dimension in traning_set_dimension_list:\n",
    "#            for popsize in popsize_list:\n",
    "#                for big_generation in big_generation_list:\n",
    "#                    input_args += f\"{seed},{dataset},{training_set_dimension},{popsize},{big_generation}\\n\"\n",
    "\n",
    "##BT-P-BG fino a 500000,100,10000, numero 43200\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{popsize},{big_generation}\\n\"\n",
    "\n",
    "##T-BP-BG fino a 50000,10000,10000, numero 62640\n",
    "#for seed in random_seeds:\n",
    "#    for dataset in datasets:\n",
    "#        for training_set_dimension in traning_set_dimension_list:\n",
    "#            for big_popsize in big_popsize_list:\n",
    "#                for big_generation in big_generation_list:\n",
    "#                    input_args += f\"{seed},{dataset},{training_set_dimension},{big_popsize},{big_generation}\\n\"\n",
    "#\n",
    "##BT-BP-BG fino a 500000,10000,10000, numero 67500\n",
    "#for seed in random_seeds:\n",
    "#    for dataset in datasets:\n",
    "#        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "#            for big_popsize in big_popsize_list:\n",
    "#                for big_generation in big_generation_list:\n",
    "#                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{big_popsize},{big_generation}\\n\"\n",
    "\n",
    "#print(input_args)\n",
    "\n",
    "with open(\"da_fare_ulysses.txt\", \"w\") as file:\n",
    "    file.write(input_args)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"random_seeds.txt\", \"r\")\n",
    "random_seeds = file.read()\n",
    "file.close()\n",
    "\n",
    "random_seeds = random_seeds.replace(\"\\n\", \"\").replace(\" \", \"\").split(\",\")\n",
    "#random_seeds = [int(seed) for seed in random_seeds]\n",
    "\n",
    "datasets_folder = './datasets/'\n",
    "datasets = ['1191_BNG_pbc.tsv', '1196_BNG_pharynx.tsv', '1595_poker.tsv']\n",
    "\n",
    "traning_set_dimension_list = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]\n",
    "big_traning_set_dimension_list = [100000, 250000, 500000]\n",
    "popsize_list = [10, 20, 50, 100]\n",
    "big_popsize_list = [200, 500, 1000, 2000, 5000, 10000]\n",
    "generation_list = [10, 20, 50, 100]\n",
    "big_generation_list = [200, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "input_args = \"\"\n",
    "\n",
    "#Finora raggiunti 26567\n",
    "\n",
    "#T-P-G fino a 50000,100,100, numero 17280 -> FATTO\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{popsize},{generation}\\n\"\n",
    "\n",
    "#BT-P-G fino a 500000,100,100, numero 21600 -> FATTO\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{popsize},{generation}\\n\"\n",
    "\n",
    "#T-BP-G fino a 50000,10000,100, numero 47520 -> IN CORSO\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{big_popsize},{generation}\\n\"\n",
    "\n",
    "#BT-BP-G fino a 500000,10000,100, numero 54000\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for generation in generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{big_popsize},{generation}\\n\"\n",
    "\n",
    "#T-P-BG fino a 50000,100,10000, numero 79920\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{popsize},{big_generation}\\n\"\n",
    "\n",
    "#BT-P-BG fino a 500000,100,10000, numero 86400\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for popsize in popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{popsize},{big_generation}\\n\"\n",
    "\n",
    "#T-BP-BG fino a 50000,10000,10000, numero 125280\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for training_set_dimension in traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{training_set_dimension},{big_popsize},{big_generation}\\n\"\n",
    "\n",
    "#BT-BP-BG fino a 500000,10000,10000, numero 135000\n",
    "for seed in random_seeds:\n",
    "    for dataset in datasets:\n",
    "        for big_training_set_dimension in big_traning_set_dimension_list:\n",
    "            for big_popsize in big_popsize_list:\n",
    "                for big_generation in big_generation_list:\n",
    "                    input_args += f\"{seed},{dataset},{big_training_set_dimension},{big_popsize},{big_generation}\\n\"\n",
    "\n",
    "#print(input_args)\n",
    "\n",
    "with open(\"input_args.txt\", \"w\") as file:\n",
    "    file.write(input_args)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('./datasets/1191_BNG_pbc.tsv', sep='\\t')\n",
    "dataset.dropna(inplace=True)\n",
    "X = dataset.drop(columns=['target'])\n",
    "y = dataset['target']\n",
    "#print(\"Currently working on \" + dataset_path + \" with seed \" + str(seed))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def divide_file(filename):\n",
    "    # Read the content of the original file\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Calculate the number of lines for each part\n",
    "    total_lines = len(lines)\n",
    "    part_size = total_lines // 4\n",
    "\n",
    "    # Remove the .txt extension from the original filename\n",
    "    base_filename = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Create and write to the new files\n",
    "    for i in range(4):\n",
    "        part_filename = f\"{base_filename}-{i+1}.txt\"\n",
    "        start_index = i * part_size\n",
    "        # Ensure the last part gets any remaining lines\n",
    "        end_index = (i + 1) * part_size if i < 3 else total_lines\n",
    "        with open(part_filename, 'w') as part_file:\n",
    "            part_file.writelines(lines[start_index:end_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide_file(\"./new_ulysses/943369.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def update_runscript():\n",
    "    # Read the original runscript.sh\n",
    "    with open('roba/runscript.sh', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Get the list of .txt files in the new_ulysses folder\n",
    "    txt_files = [f for f in os.listdir('new_ulysses') if f.endswith('.txt')]\n",
    "\n",
    "    # Update the script and save with new names\n",
    "    for txt_file in txt_files:\n",
    "        new_lines = lines.copy()\n",
    "        other_part = './new_ulysses/execute_parallel_ulysses.sh 30 '\n",
    "        new_lines[-1] = other_part + f'./new_ulysses/{txt_file}\\n'\n",
    "        # Remove the .txt extension from the txt_file name\n",
    "        base_txt_file = os.path.splitext(txt_file)[0]\n",
    "        new_script_name = f'roba/runscript_{base_txt_file}.sh'\n",
    "        with open(new_script_name, 'w') as new_file:\n",
    "            new_file.writelines(new_lines)\n",
    "\n",
    "# Call the function\n",
    "update_runscript()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def update_build_slurm():\n",
    "    # Read the original build.slurm\n",
    "    with open('roba/build.slurm', 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Get the list of runscript files in the runscripts folder\n",
    "    runscript_files = [f for f in os.listdir('runscripts') if f.startswith('runscript_') and f.endswith('.sh')]\n",
    "\n",
    "    # Update the script and save with new names\n",
    "    for runscript_file in runscript_files:\n",
    "        new_lines = lines.copy()\n",
    "        new_lines[-1] = f'singularity run general.sif ./runscripts/{runscript_file}\\n'\n",
    "        # Remove the .sh extension and 'runscript_' prefix from the runscript_file name\n",
    "        base_runscript_file = os.path.splitext(runscript_file)[0].replace('runscript_', '')\n",
    "        new_slurm_name = f'roba/build_{base_runscript_file}.slurm'\n",
    "        with open(new_slurm_name, 'w') as new_file:\n",
    "            new_file.writelines(new_lines)\n",
    "\n",
    "# Call the function\n",
    "update_build_slurm()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpgomenv-sd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
